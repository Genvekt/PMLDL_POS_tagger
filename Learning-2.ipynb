{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "Learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptArmGm_jdYQ"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "id": "ptArmGm_jdYQ",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKCIXRHIjdYc"
      },
      "source": [
        "ABSENT_WORD = (\"*****\", \"*****\")\n",
        "PADDING_CHAR = \" \"\n",
        "PADDING_WORD = \" \"\n",
        "ABSENT_PAIR = ABSENT_WORD\n",
        "WINDOW_LEN = 5\n",
        "RANDOM_CHANCE=0.1"
      ],
      "id": "tKCIXRHIjdYc",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipWbuDE8jdYd"
      },
      "source": [
        "def read_dataset(file_path, with_tags=True):\n",
        "    \"\"\"\n",
        "    Read the dataset from file\n",
        "    Args:\n",
        "        file_path (str): path to the file to read from\n",
        "        with_tags (bool): flag that indicates the presence of tags in data.\n",
        "                          Use False to read test data.\n",
        "    Returns:\n",
        "        If with_tags is true, the list of tuples, one for each sentence\n",
        "            One tuple contains list of lowercase words and corresponding list of tags\n",
        "        Othervise the list of lowercase word lists, one fo each sentence\n",
        "    \"\"\"\n",
        "    \n",
        "    dataset = []\n",
        "    with open(file_path, \"r\") as data_file:\n",
        "        for line in data_file.readlines():\n",
        "            # Split each sentence into items\n",
        "            items = line[:-1].split(\" \")\n",
        "            if with_tags:\n",
        "                # If tags are present, create separate lists of words and tags\n",
        "                words = []\n",
        "                tags = []\n",
        "                for item in items:\n",
        "                    [word, tag] = item.rsplit(\"/\", 1)\n",
        "                    words.append(word.lower())\n",
        "                    tags.append(tag)\n",
        "                dataset.append((words, tags))\n",
        "            else:\n",
        "                # If tags are not present, append word list to the dataset\n",
        "                dataset.append([word.lower() for word in items])\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def dataset_to_dictionary(dataset, absent_pair=None, absent_char=None):\n",
        "    word_to_idx = {}\n",
        "    idx_to_word = {}\n",
        "    tag_to_idx = {}\n",
        "    idx_to_tag = {}\n",
        "    \n",
        "    char_to_idx = {}\n",
        "    idx_to_char = {}\n",
        "    \n",
        "    for (words, tags) in dataset:\n",
        "        for word in words:\n",
        "            if word not in word_to_idx:\n",
        "                idx = len(word_to_idx)\n",
        "                word_to_idx[word] = idx\n",
        "                idx_to_word[idx] = word\n",
        "            for letter in word:\n",
        "                if letter not in char_to_idx:\n",
        "                    idx = len(char_to_idx)\n",
        "                    char_to_idx[letter] = idx\n",
        "                    idx_to_char[idx] = letter\n",
        "            \n",
        "        for tag in tags:\n",
        "            if tag not in tag_to_idx:\n",
        "                idx = len(tag_to_idx)\n",
        "                tag_to_idx[tag] = idx\n",
        "                idx_to_tag[idx] = tag\n",
        "                \n",
        "    if absent_pair is not None:\n",
        "        absent_word, absent_tag = absent_pair\n",
        "        if absent_word not in word_to_idx:\n",
        "            idx = len(word_to_idx)\n",
        "            word_to_idx[absent_word] = idx\n",
        "            idx_to_word[idx] = absent_word\n",
        "        if absent_tag not in tag_to_idx:\n",
        "            idx = len(tag_to_idx)\n",
        "            tag_to_idx[absent_tag] = idx\n",
        "            idx_to_tag[idx] = absent_tag\n",
        "        for letter in absent_word:\n",
        "                if letter not in char_to_idx:\n",
        "                    idx = len(char_to_idx)\n",
        "                    char_to_idx[letter] = idx\n",
        "                    idx_to_char[idx] = letter\n",
        "    if absent_char is not None and absent_char not in char_to_idx:\n",
        "        idx = len(char_to_idx)\n",
        "        char_to_idx[absent_char] = idx\n",
        "        idx_to_char[idx] = absent_char\n",
        "        \n",
        "    return word_to_idx, tag_to_idx, char_to_idx, idx_to_word, idx_to_tag, idx_to_char\n",
        "\n",
        "\n",
        "def prepare_sequence(sequence, dictionary, absent_key=None, random_key=None, random_chance=0.1):  \n",
        "    \"\"\"\n",
        "    Translate sequence according to dictionary.\n",
        "    Args:\n",
        "        sequence (list): list of keys\n",
        "        dictionary (dict): mapping from key to integer\n",
        "        absent_key (str): key which will substitute absent keys in sequence.\n",
        "                            if None, absent keys will be ignored\n",
        "        random_key (bool): key which will substitute keys in sequence \n",
        "                            with some chance (10% maybe)\n",
        "                            if None, random substitution will not be used.\n",
        "    Returns:\n",
        "        list of transformed sequence\n",
        "    \"\"\"\n",
        "    translated_seq = []\n",
        "    for key in sequence:\n",
        "        # Handle absent keys if absent_key specified\n",
        "        if key not in dictionary:\n",
        "            if absent_key is not None:\n",
        "                translated_seq.append(dictionary[absent_key])\n",
        "        # Random substitute if random_key specified\n",
        "        elif random_key is not None and torch.rand(1)[0]<random_chance:\n",
        "            translated_seq.append(dictionary[random_key])\n",
        "        else:\n",
        "            translated_seq.append(dictionary[key])\n",
        "    return torch.tensor(translated_seq, dtype=torch.long)"
      ],
      "id": "ipWbuDE8jdYd",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90jKPVqcjdYd"
      },
      "source": [
        "train_dataset = read_dataset(\"corpus.train\", with_tags=True)"
      ],
      "id": "90jKPVqcjdYd",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EDC33lmjdYe"
      },
      "source": [
        "word_to_idx, tag_to_idx, char_to_idx, idx_to_word, idx_to_tag, idx_to_char = dataset_to_dictionary(train_dataset, \n",
        "                                                                                                   absent_pair=ABSENT_PAIR, \n",
        "                                                                                                   absent_char=PADDING_CHAR)"
      ],
      "id": "_EDC33lmjdYe",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oY0moV1v798"
      },
      "source": [
        "### Data analyze"
      ],
      "id": "0oY0moV1v798"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKNbaqSQvPY7",
        "outputId": "f5d90dde-043f-418a-f361-1a34da7e3dea"
      },
      "source": [
        "max_len = 0\n",
        "max_word = ''\n",
        "lengths = []\n",
        "for (words, tags) in train_dataset:\n",
        "      for word in words:\n",
        "        lengths.append(len(word))\n",
        "        if max_len < len(word):\n",
        "          max_len = len(word)\n",
        "          max_word = word\n",
        "\n",
        "print(\"Max length word\", max_len, max_word)"
      ],
      "id": "OKNbaqSQvPY7",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max length word 54 capitalist-exploiters-greedy-american-consumers-global\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZhFTPE22WAe",
        "outputId": "1395d03f-855e-4f2f-df6a-e77ad6321804"
      },
      "source": [
        "import statistics\n",
        "mean = statistics.mean(lengths)\n",
        "print(\"Mean length:\", mean)"
      ],
      "id": "YZhFTPE22WAe",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean length: 4.455600879956665\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Bo_HrR_0kG1"
      },
      "source": [
        "##Batching"
      ],
      "id": "8Bo_HrR_0kG1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1rp-YCE5yYn"
      },
      "source": [
        "# Сортируем предложения по длине слова \n",
        "def sort_func(el, label = True):\n",
        "  if label:\n",
        "    return len(el[0])\n",
        "  return len(el)"
      ],
      "id": "U1rp-YCE5yYn",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAePywlx_Tgv"
      },
      "source": [
        "# Паддим слово пробелом\n",
        "def padd_word(word, length = 54, symbol = \" \"):\n",
        "  return word + symbol*(length - len(word))"
      ],
      "id": "aAePywlx_Tgv",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJ_rqwxqAcMV"
      },
      "source": [
        "# Падим предложение с помощью тупла padd_pair\n",
        "def padd_sentence(sentence, words_amount, padd_pair, label = True):\n",
        "  padd_word, padd_tag = padd_pair\n",
        "  for i in range(words_amount):\n",
        "    sentence[0].append(padd_word)\n",
        "    if label:\n",
        "      sentence[1].append(padd_tag)\n",
        "  return sentence"
      ],
      "id": "hJ_rqwxqAcMV",
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkJXNv7O6b6z"
      },
      "source": [
        "# Cоздание одного батча (НАДО ДОБАВИТЬ ПОДДЕРЖКУ ДАННЫХ без ЛЭЙБЛА)\n",
        "def create_batch(sentences, word_to_idx, char_to_idx, tag_to_idx, label = True):\n",
        "  max_words_num = max(len(s[0]) for s in sentences)\n",
        "  max_words_len = find_max_word_len(sentences)\n",
        "  batch_sentences = []\n",
        "  batch_taggs = []\n",
        "  batch_words = []\n",
        "  for sent in sentences:\n",
        "    padded_sent = padd_sentence(sent, max_words_num - len(sent[0]), ABSENT_PAIR)\n",
        "    words, taggs = padded_sent\n",
        "    codded_sentence = prepare_sequence(words, word_to_idx, absent_key=ABSENT_WORD[0], random_key=None)\n",
        "    codded_taggs = prepare_sequence(taggs, tag_to_idx, absent_key=ABSENT_WORD[0], random_key=None)\n",
        "    batch_sentences.append(codded_sentence)\n",
        "    batch_taggs.append(codded_taggs)\n",
        "    codded_words = []\n",
        "    for word in words:\n",
        "        paddedWord = padd_word(word, length = max_words_len, symbol=' ')\n",
        "        codded_word = prepare_sequence(paddedWord, char_to_idx, absent_key=PADDING_CHAR)\n",
        "        codded_word = torch.reshape(codded_word, (1,-1))\n",
        "        codded_words.append(codded_word)\n",
        "    words_ = torch.cat(codded_words,dim=0)\n",
        "    batch_words.append(words_)\n",
        "  batch_words = torch.stack(batch_words, dim=0)\n",
        "  batch_sentences = torch.stack(batch_sentences, dim=0)\n",
        "  batch_taggs = torch.stack(batch_taggs, dim=0)\n",
        "  return batch_sentences, batch_words, batch_taggs\n",
        "\n"
      ],
      "id": "DkJXNv7O6b6z",
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LjKqkDGu3Sl"
      },
      "source": [
        "def find_max_word_len(sentences):\n",
        "  max_words_len = 0\n",
        "  for s in sentences:\n",
        "    ws, tg = s\n",
        "    max_s= max(len(w) for w in ws)\n",
        "    if max_words_len < max_s:\n",
        "      max_words_len = max_s\n",
        "  return max_words_len"
      ],
      "id": "9LjKqkDGu3Sl",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PLyf_wD-5JT"
      },
      "source": [
        "import math\n",
        "\n",
        "class Batchizer():\n",
        "  def __init__(self, dataset, word_to_idx, char_to_idx, tag_to_idx, batch_size = 10, label = True, sorting = True):\n",
        "    batches = []\n",
        "    sentences_batches = []\n",
        "    if (sorting and label):\n",
        "      dataset.sort(key=sort_func)\n",
        "    num_batches = math.ceil(len(dataset) / batch_size )\n",
        "    for i in range(num_batches - 1):\n",
        "      if i + batch_size >= len(dataset):\n",
        "        sentences = dataset[i:len(dataset)]\n",
        "      else: \n",
        "        sentences = dataset[i:(i + batch_size)]\n",
        "      \n",
        "      batch = create_batch(sentences, word_to_idx, char_to_idx, tag_to_idx, label)\n",
        "      batches.append(batch)\n",
        "\n",
        "    self.batches = batches\n",
        "    self.word_to_idx = word_to_idx\n",
        "    self.char_to_idx = char_to_idx\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.batches)\n",
        "  \n",
        "  def get_batch(self, index):\n",
        "    return self.batches[index]\n",
        "\n"
      ],
      "id": "6PLyf_wD-5JT",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ocUVexv0rtA"
      },
      "source": [
        "## Model"
      ],
      "id": "-ocUVexv0rtA"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yWinsNx5eT8"
      },
      "source": [
        "class FinalModel(nn.Module):\n",
        "    def __init__(self, char_emb_dim, word_emb_dim, hidden_dim, vocab_size, charset_size, tagset_size, window, l):\n",
        "        super(FinalModel, self).__init__()\n",
        "        self.char_embeddings = nn.Embedding(charset_size, char_emb_dim)\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self.conv1 = nn.Conv1d(char_emb_dim, l, window, padding=(window-1)//2)\n",
        "        self.lstm = nn.LSTM(word_emb_dim+l, hidden_dim, bidirectional=True)\n",
        "        self.hidden2tag = nn.Linear(hidden_dim*2, tagset_size)\n",
        "\n",
        "    def forward(self, sentences, words, batch_size=10):\n",
        "        # Pass each window through CNN, max_pool the results for each word\n",
        "\n",
        "        words_ = words.reshape(-1, words.shape[-1])\n",
        "        # print(\"1\", words.shape, words_.shape)\n",
        "        chars_batch = self.char_embeddings(words_)\n",
        "        chars_batch = chars_batch.permute(0,2,1)\n",
        "        conv_out = self.conv1(chars_batch)\n",
        "        pool_out, _ = torch.max(conv_out, dim=2)\n",
        "        cnn_word_vecs = pool_out\n",
        "        # print(\"2\", cnn_word_vecs.shape)\n",
        "\n",
        "        sentences_ = sentences.reshape(-1)\n",
        "        # print(\"3\", sentences.shape, sentences_.shape)\n",
        "        word_embeds = self.word_embeddings(sentences_)\n",
        "        # print(\"4\", word_embeds.shape)\n",
        "        concated = torch.cat((word_embeds, cnn_word_vecs), dim=1)\n",
        "        lstm_out, _ = self.lstm(concated.view(len(sentences_), 1, -1))\n",
        "        tag_space = self.hidden2tag(lstm_out.view(len(sentences_), -1))\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        ## Решейпаем обратно в (batch_size,)\n",
        "        # tag_scores = tag_scores.reshape(batch_size, -1, tag_scores.shape[-1])\n",
        "        return tag_scores    "
      ],
      "id": "0yWinsNx5eT8",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6QdrKnX74Fa"
      },
      "source": [
        "model = FinalModel(char_emb_dim=10,\n",
        "                         word_emb_dim=10,\n",
        "                         hidden_dim=6,\n",
        "                         charset_size=len(char_to_idx),\n",
        "                         vocab_size=len(word_to_idx),\n",
        "                         tagset_size=len(tag_to_idx),\n",
        "                         window=WINDOW_LEN, \n",
        "                         l = 5)"
      ],
      "id": "d6QdrKnX74Fa",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SksiZoKG8TEv",
        "outputId": "df70baf7-dbde-4556-b8e5-2b3d04dcc830"
      },
      "source": [
        "train_dataset = read_dataset(\"corpus.train\", with_tags=True)\n",
        "batchizer = Batchizer(train_dataset, word_to_idx, char_to_idx, tag_to_idx)\n",
        "sentences, words, taggs = batchizer.get_batch(100)\n",
        "print('target:', taggs.shape)\n",
        "model(sentences,words).size()"
      ],
      "id": "SksiZoKG8TEv",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "target: torch.Size([10, 2])\n",
            "1 torch.Size([10, 2, 15]) torch.Size([20, 15])\n",
            "2 torch.Size([20, 5])\n",
            "3 torch.Size([10, 2]) torch.Size([20])\n",
            "4 torch.Size([20, 10])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20, 47])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzW4xncT3dxA",
        "outputId": "151e299c-4183-454c-fe5f-14e3201c9275"
      },
      "source": [
        "len(batchizer)"
      ],
      "id": "EzW4xncT3dxA",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3793"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEiXyEJtgWFE"
      },
      "source": [
        "### Training"
      ],
      "id": "BEiXyEJtgWFE"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxO0HigKh8_V"
      },
      "source": [
        "import torch.optim as optim"
      ],
      "id": "AxO0HigKh8_V",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "5CtBKLFWgX7G",
        "outputId": "0e18e0fe-57df-496c-ffa2-f6678f84dcd0"
      },
      "source": [
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "losses = []\n",
        "\n",
        "train_dataset = read_dataset(\"corpus.train\", with_tags=True)\n",
        "batch_size = 10\n",
        "batchizer = Batchizer(train_dataset, word_to_idx, char_to_idx, tag_to_idx, batch_size)\n",
        "\n",
        "for epoch in range(100):\n",
        "    for step in range(len(batchizer)):\n",
        "        model.zero_grad()\n",
        "\n",
        "        sentences, words, taggs = batchizer.get_batch(step)\n",
        "\n",
        "        tag_scores = model(sentences, words, batch_size)\n",
        "        # tag_scores = torch.argmax(tag_scores, dim=1) #.reshape(-1)\n",
        "        taggs = taggs.reshape(-1)\n",
        "        # print('Out:', tag_scores.shape)\n",
        "        # print('Target:', taggs.shape)\n",
        "        loss = loss_function(tag_scores, taggs)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if step % 1000 == 0:\n",
        "          print(f\"\\t loss = {losses[-1]}\")\n",
        "\n",
        "    print(f\"Epoch {epoch}: loss={losses[-1]}\")\n",
        "    torch.save(model, 'model.pth')"
      ],
      "id": "5CtBKLFWgX7G",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t loss = 1.6008665561676025\n",
            "\t loss = 0.728917121887207\n",
            "\t loss = 0.8018304705619812\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-8a74462ccb3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# print('Out:', tag_scores.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# print('Target:', taggs.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtaggs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHTrQHKHqCY4"
      },
      "source": [
        "## Validation"
      ],
      "id": "GHTrQHKHqCY4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0ELh6q7w5c7"
      },
      "source": [
        "model = torch.load('model.pth')"
      ],
      "id": "d0ELh6q7w5c7",
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hCYipH4w9uE",
        "outputId": "1153c654-77dc-4cd0-aef3-16d652028e24"
      },
      "source": [
        "model.eval()"
      ],
      "id": "-hCYipH4w9uE",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FinalModel(\n",
              "  (char_embeddings): Embedding(59, 10)\n",
              "  (word_embeddings): Embedding(38473, 10)\n",
              "  (conv1): Conv1d(10, 5, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "  (lstm): LSTM(15, 6, bidirectional=True)\n",
              "  (hidden2tag): Linear(in_features=12, out_features=47, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FeyETK3xofd"
      },
      "source": [
        "test_dataset = read_dataset(\"corpus.answer\", with_tags=True)\n",
        "batch_size = 10\n",
        "batchizer_test = Batchizer(test_dataset, word_to_idx, char_to_idx, tag_to_idx, batch_size)"
      ],
      "id": "1FeyETK3xofd",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1oMtgx3yDf0"
      },
      "source": [
        "y_true = []\n",
        "y_pred = []\n",
        "for step in range(len(batchizer_test)):\n",
        "  step = 100\n",
        "  sentences, words, taggs = batchizer_test.get_batch(step)\n",
        "  tag_scores = model(sentences, words, batch_size)\n",
        "    # tag_scores = torch.argmax(tag_scores, dim=1) #.reshape(-1)\n",
        "  taggs = taggs.reshape(-1)\n",
        "  pred = torch.argmax(tag_scores, dim=1)\n",
        "  # print(taggs.shape, pred.shape)\n",
        "  y_true += taggs.tolist()\n",
        "  y_pred += pred.tolist()\n"
      ],
      "id": "b1oMtgx3yDf0",
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iY7fzte_yKGu",
        "outputId": "28d490ed-cf1a-482a-98f9-072826b34b67"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(y_true, y_pred))"
      ],
      "id": "iY7fzte_yKGu",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tCaqX5y4nw6"
      },
      "source": [
        ""
      ],
      "id": "2tCaqX5y4nw6",
      "execution_count": null,
      "outputs": []
    }
  ]
}