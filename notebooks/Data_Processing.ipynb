{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "Data_Processing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptArmGm_jdYQ"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "id": "ptArmGm_jdYQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKCIXRHIjdYc"
      },
      "source": [
        "ABSENT_WORD = (\"*****\", \"*****\")\n",
        "PADDING_CHAR = \" \"\n",
        "ABSENT_PAIR = None\n",
        "WINDOW_LEN = 5\n",
        "RANDOM_CHANCE=0.1"
      ],
      "id": "tKCIXRHIjdYc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipWbuDE8jdYd"
      },
      "source": [
        "def read_dataset(file_path, with_tags=True):\n",
        "    \"\"\"\n",
        "    Read the dataset from file\n",
        "    Args:\n",
        "        file_path (str): path to the file to read from\n",
        "        with_tags (bool): flag that indicates the presence of tags in data.\n",
        "                          Use False to read test data.\n",
        "    Returns:\n",
        "        If with_tags is true, the list of tuples, one for each sentence\n",
        "            One tuple contains list of lowercase words and corresponding list of tags\n",
        "        Othervise the list of lowercase word lists, one fo each sentence\n",
        "    \"\"\"\n",
        "    \n",
        "    dataset = []\n",
        "    with open(file_path, \"r\") as data_file:\n",
        "        for line in data_file.readlines():\n",
        "            # Split each sentence into items\n",
        "            items = line[:-1].split(\" \")\n",
        "            if with_tags:\n",
        "                # If tags are present, create separate lists of words and tags\n",
        "                words = []\n",
        "                tags = []\n",
        "                for item in items:\n",
        "                    [word, tag] = item.rsplit(\"/\", 1)\n",
        "                    words.append(word.lower())\n",
        "                    tags.append(tag)\n",
        "                dataset.append((words, tags))\n",
        "            else:\n",
        "                # If tags are not present, append word list to the dataset\n",
        "                dataset.append([word.lower() for word in items])\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def dataset_to_dictionary(dataset, absent_pair=None, absent_char=None):\n",
        "    word_to_idx = {}\n",
        "    idx_to_word = {}\n",
        "    tag_to_idx = {}\n",
        "    idx_to_tag = {}\n",
        "    \n",
        "    char_to_idx = {}\n",
        "    idx_to_char = {}\n",
        "    \n",
        "    for (words, tags) in dataset:\n",
        "        for word in words:\n",
        "            if word not in word_to_idx:\n",
        "                idx = len(word_to_idx)\n",
        "                word_to_idx[word] = idx\n",
        "                idx_to_word[idx] = word\n",
        "            for letter in word:\n",
        "                if letter not in char_to_idx:\n",
        "                    idx = len(char_to_idx)\n",
        "                    char_to_idx[letter] = idx\n",
        "                    idx_to_char[idx] = letter\n",
        "            \n",
        "        for tag in tags:\n",
        "            if tag not in tag_to_idx:\n",
        "                idx = len(tag_to_idx)\n",
        "                tag_to_idx[tag] = idx\n",
        "                idx_to_tag[idx] = tag\n",
        "                \n",
        "    if absent_pair is not None:\n",
        "        absent_word, absent_tag = absent_pair\n",
        "        if absent_word not in word_to_idx:\n",
        "            idx = len(word_to_idx)\n",
        "            word_to_idx[absent_word] = idx\n",
        "            idx_to_word[idx] = absent_word\n",
        "        if absent_tag not in tag_to_idx:\n",
        "            idx = len(tag_to_idx)\n",
        "            tag_to_idx[absent_tag] = idx\n",
        "            idx_to_tag[idx] = absent_tag\n",
        "        for letter in absent_word:\n",
        "                if letter not in char_to_idx:\n",
        "                    idx = len(char_to_idx)\n",
        "                    char_to_idx[letter] = idx\n",
        "                    idx_to_char[idx] = letter\n",
        "    if absent_char is not None and absent_char not in char_to_idx:\n",
        "        idx = len(char_to_idx)\n",
        "        char_to_idx[absent_char] = idx\n",
        "        idx_to_char[idx] = absent_char\n",
        "        \n",
        "    return word_to_idx, tag_to_idx, char_to_idx, idx_to_word, idx_to_tag, idx_to_char\n",
        "\n",
        "\n",
        "def prepare_sequence(sequence, dictionary, absent_key=None, random_key=None, random_chance=0.1):  \n",
        "    \"\"\"\n",
        "    Translate sequence according to dictionary.\n",
        "    Args:\n",
        "        sequence (list): list of keys\n",
        "        dictionary (dict): mapping from key to integer\n",
        "        absent_key (str): key which will substitute absent keys in sequence.\n",
        "                            if None, absent keys will be ignored\n",
        "        random_key (bool): key which will substitute keys in sequence \n",
        "                            with some chance (10% maybe)\n",
        "                            if None, random substitution will not be used.\n",
        "    Returns:\n",
        "        list of transformed sequence\n",
        "    \"\"\"\n",
        "    translated_seq = []\n",
        "    for key in sequence:\n",
        "        # Handle absent keys if absent_key specified\n",
        "        if key not in dictionary:\n",
        "            if absent_key is not None:\n",
        "                translated_seq.append(dictionary[absent_key])\n",
        "        # Random substitute if random_key specified\n",
        "        elif random_key is not None and torch.rand(1)[0]<random_chance:\n",
        "            translated_seq.append(dictionary[random_key])\n",
        "        else:\n",
        "            translated_seq.append(dictionary[key])\n",
        "    return torch.tensor(translated_seq, dtype=torch.long)"
      ],
      "id": "ipWbuDE8jdYd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90jKPVqcjdYd"
      },
      "source": [
        "train_dataset = read_dataset(\"corpus.train\", with_tags=True)"
      ],
      "id": "90jKPVqcjdYd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EDC33lmjdYe"
      },
      "source": [
        "word_to_idx, tag_to_idx, char_to_idx, idx_to_word, idx_to_tag, idx_to_char = dataset_to_dictionary(train_dataset, \n",
        "                                                                                                   absent_pair=ABSENT_PAIR, \n",
        "                                                                                                   absent_char=PADDING_CHAR)"
      ],
      "id": "_EDC33lmjdYe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FU4QPABv7SD"
      },
      "source": [
        "def paddWord(word, length = 54):\n",
        "  return word + \" \"*(length - len(word))"
      ],
      "id": "2FU4QPABv7SD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlQJrwUgwoPn"
      },
      "source": [
        ""
      ],
      "id": "QlQJrwUgwoPn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oY0moV1v798"
      },
      "source": [
        "### Data analyze"
      ],
      "id": "0oY0moV1v798"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKNbaqSQvPY7",
        "outputId": "fcc98549-2fbf-4d8c-98e3-cbb6ed6471fd"
      },
      "source": [
        "max_len = 0\n",
        "max_word = ''\n",
        "lengths = []\n",
        "for (words, tags) in train_dataset:\n",
        "      for word in words:\n",
        "        lengths.append(len(word))\n",
        "        if max_len < len(word):\n",
        "          max_len = len(word)\n",
        "          max_word = word\n",
        "\n",
        "print(\"Max length word\", max_len, max_word)"
      ],
      "id": "OKNbaqSQvPY7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max length word 38 bioequivalence-therapeutic-equivalence\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZhFTPE22WAe",
        "outputId": "35fc82c4-688d-4bf6-8a8e-a60685506888"
      },
      "source": [
        "import statistics\n",
        "mean = statistics.mean(lengths)\n",
        "print(\"Mean length:\", mean)"
      ],
      "id": "YZhFTPE22WAe",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean length: 4.4551791689662545\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Bo_HrR_0kG1"
      },
      "source": [
        "##Batching"
      ],
      "id": "8Bo_HrR_0kG1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "Ubp_O4Ri0m4O",
        "outputId": "4de6b430-f182-4df3-c0a2-96379b525af8"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class ComplicatedDataset(Dataset):\n",
        "   def __init__(self, X, Y):\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "    if len(self.X) != len(self.Y):\n",
        "      raise Exception(\"The length of X does not match the length of Y\")\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # note that this isn't randomly selecting. It's a simple get a single item that represents an x and y\n",
        "    _x = self.X[index]\n",
        "    _y = self.Y[index]\n",
        "\n",
        "    return _x, _y"
      ],
      "id": "Ubp_O4Ri0m4O",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-f85e086b626d>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    class TimeSeriesDataSet(Dataset):\u001b[0m\n\u001b[0m                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ocUVexv0rtA"
      },
      "source": [
        "## Model"
      ],
      "id": "-ocUVexv0rtA"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulT4Bzy6jdYe"
      },
      "source": [
        "class ComplicatedModel(nn.Module):\n",
        "    def __init__(self, char_emb_dim, word_emb_dim, hidden_dim, vocab_size, charset_size, tagset_size, window, l):\n",
        "        super(ComplicatedModel, self).__init__()\n",
        "        self.char_embeddings = nn.Embedding(charset_size, char_emb_dim)\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        \n",
        "        self.conv1 = nn.Conv1d(char_emb_dim, l, window, padding=(window-1)//2)\n",
        "        \n",
        "        self.lstm = nn.LSTM(word_emb_dim+l, hidden_dim, bidirectional=True)\n",
        "        self.hidden2tag = nn.Linear(hidden_dim*2, tagset_size)\n",
        "    def forward(self, sentence, words):\n",
        "        # Pass each window through CNN, max_pool the results for each word\n",
        "\n",
        "\n",
        "        cnn_word_vecs = []\n",
        "        for chars in words:\n",
        "            # print(\"\\nchars\", chars)\n",
        "            chars_batch = self.char_embeddings(chars)\n",
        "            # print(\"\\nchars_batch\", chars_batch)\n",
        "            chars_batch = chars_batch.permute(0,2,1)\n",
        "            \n",
        "            conv_out = self.conv1(chars_batch)\n",
        "            print(conv_out.size())\n",
        "            pool_out, _ = torch.max(conv_out, dim=2)\n",
        "            print(pool_out.size(), '\\n')\n",
        "            pool_out = torch.reshape(pool_out, (-1,))\n",
        "            \n",
        "            cnn_word_vecs.append(pool_out)\n",
        "            \n",
        "        cnn_word_vecs = torch.stack(cnn_word_vecs)\n",
        "        word_embeds = self.word_embeddings(sentence)\n",
        "    \n",
        "        concated = torch.cat((word_embeds, cnn_word_vecs), dim=1)\n",
        "        lstm_out, _ = self.lstm(concated.view(len(sentence), 1, -1))\n",
        "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores        "
      ],
      "id": "ulT4Bzy6jdYe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN6g3w9NjdYe"
      },
      "source": [
        "modelOld = ComplicatedModel(char_emb_dim=10,\n",
        "                         word_emb_dim=10,\n",
        "                         hidden_dim=6,\n",
        "                         charset_size=len(char_to_idx),\n",
        "                         vocab_size=len(char_to_idx),\n",
        "                         tagset_size=len(tag_to_idx),\n",
        "                         window=WINDOW_LEN, \n",
        "                         l = 5)"
      ],
      "id": "NN6g3w9NjdYe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFTEkPfCjdYf"
      },
      "source": [
        "for sentence in train_dataset:\n",
        "    words, taggs = sentence\n",
        "    codded_sentence = prepare_sequence(words, word_to_idx, absent_key=ABSENT_WORD[0], random_key=None)\n",
        "    codded_words = []\n",
        "    for word in words:\n",
        "        codded_word = prepare_sequence(word, char_to_idx, absent_key=PADDING_CHAR)\n",
        "        codded_word = torch.reshape(codded_word, (1,-1))\n",
        "        codded_words.append(codded_word)\n",
        "    print(modelOld(codded_sentence, codded_words).size())\n",
        "    break\n"
      ],
      "id": "rFTEkPfCjdYf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmLcS3y46zKF"
      },
      "source": [
        "class VeryComplicatedModel(nn.Module):\n",
        "    def __init__(self, char_emb_dim, word_emb_dim, hidden_dim, vocab_size, charset_size, tagset_size, window, l):\n",
        "        super(VeryComplicatedModel, self).__init__()\n",
        "        self.char_embeddings = nn.Embedding(charset_size, char_emb_dim)\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        \n",
        "        self.conv1 = nn.Conv1d(char_emb_dim, l, window, padding=(window-1)//2)\n",
        "        \n",
        "        self.lstm = nn.LSTM(word_emb_dim+l, hidden_dim, bidirectional=True)\n",
        "        self.hidden2tag = nn.Linear(hidden_dim*2, tagset_size)\n",
        "\n",
        "    def forward(self, sentence, words):\n",
        "        # Pass each window through CNN, max_pool the results for each word\n",
        "\n",
        "        cnn_word_vecs = []\n",
        "        chars_batch = self.char_embeddings(words)\n",
        "        chars_batch = chars_batch.permute(0,2,1)\n",
        "        conv_out = self.conv1(chars_batch)\n",
        "        pool_out, _ = torch.max(conv_out, dim=2)\n",
        "        cnn_word_vecs = pool_out\n",
        "  \n",
        "        word_embeds = self.word_embeddings(sentence)\n",
        "        concated = torch.cat((word_embeds, cnn_word_vecs), dim=1)\n",
        "        lstm_out, _ = self.lstm(concated.view(len(sentence), 1, -1))\n",
        "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores    "
      ],
      "id": "wmLcS3y46zKF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rpHhoFY62oM"
      },
      "source": [
        "model = VeryComplicatedModel(char_emb_dim=10,\n",
        "                         word_emb_dim=10,\n",
        "                         hidden_dim=6,\n",
        "                         charset_size=len(char_to_idx),\n",
        "                         vocab_size=len(word_to_idx),\n",
        "                         tagset_size=len(tag_to_idx),\n",
        "                         window=WINDOW_LEN, \n",
        "                         l = 5)"
      ],
      "id": "2rpHhoFY62oM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1AcllCCjdYf",
        "outputId": "01090583-e16b-4a84-c6e4-0caf62014338"
      },
      "source": [
        "for sentence in train_dataset:\n",
        "    words, taggs = sentence\n",
        "    codded_sentence = prepare_sequence(words, word_to_idx, absent_key=ABSENT_WORD[0], random_key=None)\n",
        "    codded_words = []\n",
        "    for word in words:\n",
        "        paddedWord = paddWord(word)\n",
        "        codded_word = prepare_sequence(paddedWord, char_to_idx, absent_key=PADDING_CHAR)\n",
        "        codded_word = torch.reshape(codded_word, (1,-1))\n",
        "        codded_words.append(codded_word)\n",
        "    words_ = torch.cat(codded_words,dim=0)\n",
        "    # print(words_.size(), len(codded_words), words_)\n",
        "    print(model(codded_sentence, words_).size())\n",
        "    break\n"
      ],
      "id": "w1AcllCCjdYf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([49, 46])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEiXyEJtgWFE"
      },
      "source": [
        "### Training"
      ],
      "id": "BEiXyEJtgWFE"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxO0HigKh8_V"
      },
      "source": [
        "import torch.optim as optim"
      ],
      "id": "AxO0HigKh8_V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CtBKLFWgX7G",
        "outputId": "19d19b0e-878e-4333-dec1-b5a1294c41fb"
      },
      "source": [
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.025)\n",
        "losses = []\n",
        "\n",
        "for epoch in range(10):\n",
        "    for (idx, sentence) in enumerate(train_dataset):\n",
        "        model.zero_grad()\n",
        "\n",
        "        words, taggs = sentence\n",
        "\n",
        "        target = prepare_sequence(taggs, tag_to_idx, absent_key=ABSENT_WORD[0], random_key=None) #[tag_to_idx[tag] for tag in taggs ]\n",
        "        # target = torch.tensor(codded_tags, dtype=torch.long)\n",
        "\n",
        "        codded_sentence = prepare_sequence(words, word_to_idx, absent_key=ABSENT_WORD[0], random_key=None)\n",
        "        codded_words = []\n",
        "        for word in words:\n",
        "          paddedWord = paddWord(word)\n",
        "          codded_word = prepare_sequence(paddedWord, char_to_idx, absent_key=PADDING_CHAR)\n",
        "          codded_word = torch.reshape(codded_word, (1,-1))\n",
        "          codded_words.append(codded_word)\n",
        "\n",
        "        words_ = torch.cat(codded_words,dim=0)\n",
        "        tag_scores = model(codded_sentence, words_)\n",
        "        \n",
        "        loss = loss_function(tag_scores, target)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # if idx % 100 == 0:\n",
        "          # print(f\"\\t loss = {losses[-1]}\")\n",
        "\n",
        "    print(f\"Epoch {epoch}: loss={losses[-1]}\")\n",
        "    torch.save(model, 'model.pth')"
      ],
      "id": "5CtBKLFWgX7G",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: loss=2.208832025527954\n",
            "Epoch 1: loss=1.9007292985916138\n",
            "Epoch 2: loss=1.8292826414108276\n",
            "Epoch 3: loss=1.8674302101135254\n",
            "Epoch 4: loss=1.8857098817825317\n",
            "Epoch 5: loss=1.9156347513198853\n",
            "Epoch 6: loss=1.9335263967514038\n",
            "Epoch 7: loss=1.9371687173843384\n",
            "Epoch 8: loss=1.9675663709640503\n",
            "Epoch 9: loss=1.9892007112503052\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8amXssBkvTL"
      },
      "source": [
        ""
      ],
      "id": "b8amXssBkvTL",
      "execution_count": null,
      "outputs": []
    }
  ]
}