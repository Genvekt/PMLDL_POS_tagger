{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "DataProcessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptArmGm_jdYQ"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "id": "ptArmGm_jdYQ",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKCIXRHIjdYc"
      },
      "source": [
        "ABSENT_WORD = (\"*****\", \"*****\")\n",
        "PADDING_CHAR = \" \"\n",
        "PADDING_WORD = \" \"\n",
        "ABSENT_PAIR = ABSENT_WORD\n",
        "WINDOW_LEN = 5\n",
        "RANDOM_CHANCE=0.1"
      ],
      "id": "tKCIXRHIjdYc",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipWbuDE8jdYd"
      },
      "source": [
        "def read_dataset(file_path, with_tags=True):\n",
        "    \"\"\"\n",
        "    Read the dataset from file\n",
        "    Args:\n",
        "        file_path (str): path to the file to read from\n",
        "        with_tags (bool): flag that indicates the presence of tags in data.\n",
        "                          Use False to read test data.\n",
        "    Returns:\n",
        "        If with_tags is true, the list of tuples, one for each sentence\n",
        "            One tuple contains list of lowercase words and corresponding list of tags\n",
        "        Othervise the list of lowercase word lists, one fo each sentence\n",
        "    \"\"\"\n",
        "    \n",
        "    dataset = []\n",
        "    with open(file_path, \"r\") as data_file:\n",
        "        for line in data_file.readlines():\n",
        "            # Split each sentence into items\n",
        "            items = line[:-1].split(\" \")\n",
        "            if with_tags:\n",
        "                # If tags are present, create separate lists of words and tags\n",
        "                words = []\n",
        "                tags = []\n",
        "                for item in items:\n",
        "                    [word, tag] = item.rsplit(\"/\", 1)\n",
        "                    words.append(word.lower())\n",
        "                    tags.append(tag)\n",
        "                dataset.append((words, tags))\n",
        "            else:\n",
        "                # If tags are not present, append word list to the dataset\n",
        "                dataset.append([word.lower() for word in items])\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def dataset_to_dictionary(dataset, absent_pair=None, absent_char=None):\n",
        "    word_to_idx = {}\n",
        "    idx_to_word = {}\n",
        "    tag_to_idx = {}\n",
        "    idx_to_tag = {}\n",
        "    \n",
        "    char_to_idx = {}\n",
        "    idx_to_char = {}\n",
        "    \n",
        "    for (words, tags) in dataset:\n",
        "        for word in words:\n",
        "            if word not in word_to_idx:\n",
        "                idx = len(word_to_idx)\n",
        "                word_to_idx[word] = idx\n",
        "                idx_to_word[idx] = word\n",
        "            for letter in word:\n",
        "                if letter not in char_to_idx:\n",
        "                    idx = len(char_to_idx)\n",
        "                    char_to_idx[letter] = idx\n",
        "                    idx_to_char[idx] = letter\n",
        "            \n",
        "        for tag in tags:\n",
        "            if tag not in tag_to_idx:\n",
        "                idx = len(tag_to_idx)\n",
        "                tag_to_idx[tag] = idx\n",
        "                idx_to_tag[idx] = tag\n",
        "                \n",
        "    if absent_pair is not None:\n",
        "        absent_word, absent_tag = absent_pair\n",
        "        if absent_word not in word_to_idx:\n",
        "            idx = len(word_to_idx)\n",
        "            word_to_idx[absent_word] = idx\n",
        "            idx_to_word[idx] = absent_word\n",
        "        if absent_tag not in tag_to_idx:\n",
        "            idx = len(tag_to_idx)\n",
        "            tag_to_idx[absent_tag] = idx\n",
        "            idx_to_tag[idx] = absent_tag\n",
        "        for letter in absent_word:\n",
        "                if letter not in char_to_idx:\n",
        "                    idx = len(char_to_idx)\n",
        "                    char_to_idx[letter] = idx\n",
        "                    idx_to_char[idx] = letter\n",
        "    if absent_char is not None and absent_char not in char_to_idx:\n",
        "        idx = len(char_to_idx)\n",
        "        char_to_idx[absent_char] = idx\n",
        "        idx_to_char[idx] = absent_char\n",
        "        \n",
        "    return word_to_idx, tag_to_idx, char_to_idx, idx_to_word, idx_to_tag, idx_to_char\n",
        "\n",
        "\n",
        "def prepare_sequence(sequence, dictionary, absent_key=None, random_key=None, random_chance=0.1):  \n",
        "    \"\"\"\n",
        "    Translate sequence according to dictionary.\n",
        "    Args:\n",
        "        sequence (list): list of keys\n",
        "        dictionary (dict): mapping from key to integer\n",
        "        absent_key (str): key which will substitute absent keys in sequence.\n",
        "                            if None, absent keys will be ignored\n",
        "        random_key (bool): key which will substitute keys in sequence \n",
        "                            with some chance (10% maybe)\n",
        "                            if None, random substitution will not be used.\n",
        "    Returns:\n",
        "        list of transformed sequence\n",
        "    \"\"\"\n",
        "    translated_seq = []\n",
        "    for key in sequence:\n",
        "        # Handle absent keys if absent_key specified\n",
        "        if key not in dictionary:\n",
        "            if absent_key is not None:\n",
        "                translated_seq.append(dictionary[absent_key])\n",
        "        # Random substitute if random_key specified\n",
        "        elif random_key is not None and torch.rand(1)[0]<random_chance:\n",
        "            translated_seq.append(dictionary[random_key])\n",
        "        else:\n",
        "            translated_seq.append(dictionary[key])\n",
        "    return torch.tensor(translated_seq, dtype=torch.long)"
      ],
      "id": "ipWbuDE8jdYd",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90jKPVqcjdYd"
      },
      "source": [
        "train_dataset = read_dataset(\"corpus.train\", with_tags=True)"
      ],
      "id": "90jKPVqcjdYd",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EDC33lmjdYe"
      },
      "source": [
        "word_to_idx, tag_to_idx, char_to_idx, idx_to_word, idx_to_tag, idx_to_char = dataset_to_dictionary(train_dataset, \n",
        "                                                                                                   absent_pair=ABSENT_PAIR, \n",
        "                                                                                                   absent_char=PADDING_CHAR)"
      ],
      "id": "_EDC33lmjdYe",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlQJrwUgwoPn"
      },
      "source": [
        ""
      ],
      "id": "QlQJrwUgwoPn",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oY0moV1v798"
      },
      "source": [
        "### Data analyze"
      ],
      "id": "0oY0moV1v798"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKNbaqSQvPY7",
        "outputId": "f5d90dde-043f-418a-f361-1a34da7e3dea"
      },
      "source": [
        "max_len = 0\n",
        "max_word = ''\n",
        "lengths = []\n",
        "for (words, tags) in train_dataset:\n",
        "      for word in words:\n",
        "        lengths.append(len(word))\n",
        "        if max_len < len(word):\n",
        "          max_len = len(word)\n",
        "          max_word = word\n",
        "\n",
        "print(\"Max length word\", max_len, max_word)"
      ],
      "id": "OKNbaqSQvPY7",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max length word 54 capitalist-exploiters-greedy-american-consumers-global\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZhFTPE22WAe",
        "outputId": "1395d03f-855e-4f2f-df6a-e77ad6321804"
      },
      "source": [
        "import statistics\n",
        "mean = statistics.mean(lengths)\n",
        "print(\"Mean length:\", mean)"
      ],
      "id": "YZhFTPE22WAe",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean length: 4.455600879956665\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Bo_HrR_0kG1"
      },
      "source": [
        "##Batching"
      ],
      "id": "8Bo_HrR_0kG1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rj5pJRPj56_U"
      },
      "source": [
        "for (idx, sent) in enumerate(train_dataset):\n",
        "  print(f'{idx}: len = {len(sent[0])}')"
      ],
      "id": "Rj5pJRPj56_U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1rp-YCE5yYn"
      },
      "source": [
        "def sort_func(el):\n",
        "  return len(el[0])"
      ],
      "id": "U1rp-YCE5yYn",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAePywlx_Tgv"
      },
      "source": [
        "def padd_word(word, length = 54, symbol = \" \"):\n",
        "  return word + symbol*(length - len(word))"
      ],
      "id": "aAePywlx_Tgv",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJ_rqwxqAcMV"
      },
      "source": [
        "def padd_sentence(sentence, words_amount, padd_pair):\n",
        "  padd_word, padd_tag = padd_pair\n",
        "  # print(\"Padding...\", padd_word, padd_tag)\n",
        "  for i in range(words_amount):\n",
        "    sentence[0].append(padd_word)\n",
        "    sentence[1].append(padd_tag)\n",
        "  return sentence"
      ],
      "id": "hJ_rqwxqAcMV",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTjVXeTTpMea"
      },
      "source": [
        "train_dataset = read_dataset(\"corpus.train\", with_tags=True)\n",
        "train_dataset.sort(key=sort_func)"
      ],
      "id": "MTjVXeTTpMea",
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkJXNv7O6b6z"
      },
      "source": [
        "def create_batch(sentences, word_to_idx, char_to_idx, tag_to_idx, label = True):\n",
        "  max_words_num = max(len(s[0]) for s in sentences)\n",
        "  max_words_len = find_max_word_len(sentences)\n",
        "  batch_sentences = []\n",
        "  batch_taggs = []\n",
        "  batch_words = []\n",
        "  # print('MAX WORD LEN:', max_words_len)\n",
        "  # print('MAX WORD NUM:', max_words_num)\n",
        "  for sent in sentences:\n",
        "    padded_sent = padd_sentence(sent, max_words_num - len(sent[0]), ABSENT_PAIR)\n",
        "    # print('\\nWordsNum:', len(padded_sent[0]))\n",
        "    words, taggs = padded_sent\n",
        "    codded_sentence = prepare_sequence(words, word_to_idx, absent_key=ABSENT_WORD[0], random_key=None)\n",
        "    codded_taggs = prepare_sequence(taggs, tag_to_idx, absent_key=ABSENT_WORD[0], random_key=None)\n",
        "    # print(codded_sentence.shape)\n",
        "    batch_sentences.append(codded_sentence)\n",
        "    batch_taggs.append(codded_taggs)\n",
        "    codded_words = []\n",
        "    for word in words:\n",
        "        paddedWord = padd_word(word, length = max_words_len, symbol=' ')\n",
        "        codded_word = prepare_sequence(paddedWord, char_to_idx, absent_key=PADDING_CHAR)\n",
        "        codded_word = torch.reshape(codded_word, (1,-1))\n",
        "        codded_words.append(codded_word)\n",
        "    words_ = torch.cat(codded_words,dim=0)\n",
        "    batch_words.append(words_)\n",
        "    # print( batch_words[-1].size(), batch_words[-1])\n",
        "  # print(len(batch_sentences))\n",
        "  batch_words = torch.stack(batch_words, dim=0)\n",
        "  batch_sentences = torch.stack(batch_sentences, dim=0)\n",
        "  batch_taggs = torch.stack(batch_taggs, dim=0)\n",
        "  # print(batch_sentences.shape)\n",
        "  # print(batch_words.size(), batch_senteces.size())\n",
        "  return batch_sentences, batch_words, batch_taggs\n",
        "\n"
      ],
      "id": "DkJXNv7O6b6z",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LjKqkDGu3Sl"
      },
      "source": [
        "def find_max_word_len(sentences):\n",
        "  max_words_len = 0\n",
        "  for s in sentences:\n",
        "    ws, tg = s\n",
        "    max_s= max(len(w) for w in ws)\n",
        "    if max_words_len < max_s:\n",
        "      max_words_len = max_s\n",
        "  return max_words_len"
      ],
      "id": "9LjKqkDGu3Sl",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PLyf_wD-5JT"
      },
      "source": [
        "import math\n",
        "\n",
        "class Batchizer():\n",
        "  def __init__(self, dataset, word_to_idx, char_to_idx, tag_to_idx, batch_size = 10, label = True, sorting = True):\n",
        "    batches = []\n",
        "    sentences_batches = []\n",
        "    if (sorting and label):\n",
        "      dataset.sort(key=sort_func)\n",
        "    num_batches = math.ceil(len(dataset) / batch_size )\n",
        "    for i in range(num_batches - 1):\n",
        "      if i + batch_size >= len(dataset):\n",
        "        sentences = dataset[i:len(dataset)]\n",
        "      else: \n",
        "        sentences = dataset[i:(i + batch_size)]\n",
        "      \n",
        "      batch = create_batch(sentences, word_to_idx, char_to_idx, tag_to_idx, label)\n",
        "      batches.append(batch)\n",
        "\n",
        "    self.batches = batches\n",
        "    self.word_to_idx = word_to_idx\n",
        "    self.char_to_idx = char_to_idx\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.batches)\n",
        "  \n",
        "  def get_batch(self, index):\n",
        "    return self.batches[index]\n",
        "\n"
      ],
      "id": "6PLyf_wD-5JT",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7zHK0qPyej6"
      },
      "source": [
        "train_dataset = read_dataset(\"corpus.train\", with_tags=True)\n",
        "batchizer = Batchizer(train_dataset, word_to_idx, char_to_idx, tag_to_idx,)"
      ],
      "id": "M7zHK0qPyej6",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jp8K80iGzKMR",
        "outputId": "9a7ceecf-5e7d-4db0-f2ef-49c00790f298"
      },
      "source": [
        "sentences, words, taggs = batchizer.get_batch(1)\n",
        "print(words.size(), sentences.size(), taggs.size())"
      ],
      "id": "jp8K80iGzKMR",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 1, 14]) torch.Size([10, 1]) torch.Size([10, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ocUVexv0rtA"
      },
      "source": [
        "## Model"
      ],
      "id": "-ocUVexv0rtA"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmLcS3y46zKF"
      },
      "source": [
        "class VeryComplicatedModel(nn.Module):\n",
        "    def __init__(self, char_emb_dim, word_emb_dim, hidden_dim, vocab_size, charset_size, tagset_size, window, l):\n",
        "        super(VeryComplicatedModel, self).__init__()\n",
        "        self.char_embeddings = nn.Embedding(charset_size, char_emb_dim)\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        \n",
        "        self.conv1 = nn.Conv1d(char_emb_dim, l, window, padding=(window-1)//2)\n",
        "        \n",
        "        self.lstm = nn.LSTM(word_emb_dim+l, hidden_dim, bidirectional=True)\n",
        "        self.hidden2tag = nn.Linear(hidden_dim*2, tagset_size)\n",
        "\n",
        "    def forward(self, sentence, words):\n",
        "        # Pass each window through CNN, max_pool the results for each word\n",
        "\n",
        "        cnn_word_vecs = []\n",
        "        chars_batch = self.char_embeddings(words)\n",
        "        chars_batch = chars_batch.permute(0,2,1)\n",
        "        conv_out = self.conv1(chars_batch)\n",
        "        pool_out, _ = torch.max(conv_out, dim=2)\n",
        "        cnn_word_vecs = pool_out\n",
        "  \n",
        "        word_embeds = self.word_embeddings(sentence)\n",
        "        concated = torch.cat((word_embeds, cnn_word_vecs), dim=1)\n",
        "        lstm_out, _ = self.lstm(concated.view(len(sentence), 1, -1))\n",
        "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores    "
      ],
      "id": "wmLcS3y46zKF",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rpHhoFY62oM"
      },
      "source": [
        "model = VeryComplicatedModel(char_emb_dim=10,\n",
        "                         word_emb_dim=10,\n",
        "                         hidden_dim=6,\n",
        "                         charset_size=len(char_to_idx),\n",
        "                         vocab_size=len(word_to_idx),\n",
        "                         tagset_size=len(tag_to_idx),\n",
        "                         window=WINDOW_LEN, \n",
        "                         l = 5)"
      ],
      "id": "2rpHhoFY62oM",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1AcllCCjdYf",
        "outputId": "e9f5e100-364d-4ed0-c93e-b970399afb17"
      },
      "source": [
        "for sentence in train_dataset:\n",
        "    words, taggs = sentence\n",
        "    codded_sentence = prepare_sequence(words, word_to_idx, absent_key=ABSENT_WORD[0], random_key=None)\n",
        "    codded_words = []\n",
        "    max_word_len = max(len(word) for word in words)\n",
        "    for word in words:\n",
        "        paddedWord = padd_word(word, length = max_word_len, symbol=' ')\n",
        "        codded_word = prepare_sequence(paddedWord, char_to_idx, absent_key=PADDING_CHAR)\n",
        "        codded_word = torch.reshape(codded_word, (1,-1))\n",
        "        codded_words.append(codded_word)\n",
        "    words_ = torch.cat(codded_words,dim=0)\n",
        "    # print(words_.size(), len(codded_words), words_)\n",
        "    print('Input words', words_.size())\n",
        "    print(model(codded_sentence, words_).size())\n",
        "    break\n"
      ],
      "id": "w1AcllCCjdYf",
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input words torch.Size([49, 11])\n",
            "torch.Size([49, 47])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yWinsNx5eT8"
      },
      "source": [
        "class FinalModel(nn.Module):\n",
        "    def __init__(self, char_emb_dim, word_emb_dim, hidden_dim, vocab_size, charset_size, tagset_size, window, l):\n",
        "        super(FinalModel, self).__init__()\n",
        "        self.char_embeddings = nn.Embedding(charset_size, char_emb_dim)\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self.conv1 = nn.Conv1d(char_emb_dim, l, window, padding=(window-1)//2)\n",
        "        self.lstm = nn.LSTM(word_emb_dim+l, hidden_dim, bidirectional=True)\n",
        "        self.hidden2tag = nn.Linear(hidden_dim*2, tagset_size)\n",
        "\n",
        "    def forward(self, sentences, words, batch_size=10):\n",
        "        # Pass each window through CNN, max_pool the results for each word\n",
        "\n",
        "        words_ = words.reshape(-1, words.shape[-1])\n",
        "        print(\"1\", words.shape, words_.shape)\n",
        "        chars_batch = self.char_embeddings(words_)\n",
        "        chars_batch = chars_batch.permute(0,2,1)\n",
        "        conv_out = self.conv1(chars_batch)\n",
        "        pool_out, _ = torch.max(conv_out, dim=2)\n",
        "        cnn_word_vecs = pool_out\n",
        "        print(\"2\", cnn_word_vecs.shape)\n",
        "\n",
        "        sentences_ = sentences.reshape(-1)\n",
        "        print(\"3\", sentences.shape, sentences_.shape)\n",
        "        word_embeds = self.word_embeddings(sentences_)\n",
        "        print(\"4\", word_embeds.shape)\n",
        "        concated = torch.cat((word_embeds, cnn_word_vecs), dim=1)\n",
        "        lstm_out, _ = self.lstm(concated.view(len(sentences_), 1, -1))\n",
        "        tag_space = self.hidden2tag(lstm_out.view(len(sentences_), -1))\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        tag_scores = tag_scores.reshape(batch_size, -1, tag_scores.shape[-1])\n",
        "        return tag_scores    "
      ],
      "id": "0yWinsNx5eT8",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6QdrKnX74Fa"
      },
      "source": [
        "model = FinalModel(char_emb_dim=10,\n",
        "                         word_emb_dim=10,\n",
        "                         hidden_dim=6,\n",
        "                         charset_size=len(char_to_idx),\n",
        "                         vocab_size=len(word_to_idx),\n",
        "                         tagset_size=len(tag_to_idx),\n",
        "                         window=WINDOW_LEN, \n",
        "                         l = 5)"
      ],
      "id": "d6QdrKnX74Fa",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SksiZoKG8TEv",
        "outputId": "2bcee958-ab0a-4485-b10f-1e5345ef184d"
      },
      "source": [
        "train_dataset = read_dataset(\"corpus.train\", with_tags=True)\n",
        "batchizer = Batchizer(train_dataset, word_to_idx, char_to_idx, tag_to_idx)\n",
        "sentences, words, taggs = batchizer.get_batch(100)\n",
        "model(sentences,words).size()"
      ],
      "id": "SksiZoKG8TEv",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 torch.Size([10, 2, 15]) torch.Size([20, 15])\n",
            "2 torch.Size([20, 5])\n",
            "3 torch.Size([10, 2]) torch.Size([20])\n",
            "4 torch.Size([20, 10])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 2, 47])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzW4xncT3dxA",
        "outputId": "0b17e0d9-9e8e-498c-efad-cb5799f6b330"
      },
      "source": [
        "len(batchizer)"
      ],
      "id": "EzW4xncT3dxA",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3793"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEiXyEJtgWFE"
      },
      "source": [
        "### Training"
      ],
      "id": "BEiXyEJtgWFE"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxO0HigKh8_V"
      },
      "source": [
        "import torch.optim as optim"
      ],
      "id": "AxO0HigKh8_V",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "5CtBKLFWgX7G",
        "outputId": "180191d2-e872-4a6a-be2d-0169a9d864bd"
      },
      "source": [
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "losses = []\n",
        "\n",
        "train_dataset = read_dataset(\"corpus.train\", with_tags=True)\n",
        "batch_size = 10\n",
        "batchizer = Batchizer(train_dataset, word_to_idx, char_to_idx, tag_to_idx, batch_size)\n",
        "\n",
        "for epoch in range(100):\n",
        "    for step in range(len(batchizer)):\n",
        "        model.zero_grad()\n",
        "\n",
        "        sentences, words, taggs = batchizer.get_batch(step)\n",
        "\n",
        "        tag_scores = model(sentences, words, batch_size)\n",
        "        # pred = torch.argmax(tag_scores, dim=2) #.reshape(-1)\n",
        "        # print(pred)\n",
        "        print(tag_scores.shape)\n",
        "        loss = loss_function(tag_scores, taggs)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if step % 100 == 0:\n",
        "          print(f\"\\t loss = {losses[-1]}\")\n",
        "\n",
        "    print(f\"Epoch {epoch}: loss={losses[-1]}\")\n",
        "    torch.save(model, 'model.pth')"
      ],
      "id": "5CtBKLFWgX7G",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 torch.Size([10, 1, 14]) torch.Size([10, 14])\n",
            "2 torch.Size([10, 5])\n",
            "3 torch.Size([10, 1]) torch.Size([10])\n",
            "4 torch.Size([10, 10])\n",
            "torch.Size([10, 1, 47])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-e3ffab3d0cce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# print(pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtaggs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2273\u001b[0m             raise ValueError('Expected target size {}, got {}'.format(\n\u001b[0;32m-> 2274\u001b[0;31m                 out_size, target.size()))\n\u001b[0m\u001b[1;32m   2275\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2276\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected target size (10, 47), got torch.Size([10, 1])"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHTrQHKHqCY4"
      },
      "source": [
        "## Validation"
      ],
      "id": "GHTrQHKHqCY4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0ELh6q7w5c7"
      },
      "source": [
        "model = torch.load('model.pth')"
      ],
      "id": "d0ELh6q7w5c7",
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hCYipH4w9uE",
        "outputId": "5656adc0-fe86-4c68-a5ee-f8bd5b76bafe"
      },
      "source": [
        "model.eval()"
      ],
      "id": "-hCYipH4w9uE",
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VeryComplicatedModel(\n",
              "  (char_embeddings): Embedding(59, 10)\n",
              "  (word_embeddings): Embedding(38473, 10)\n",
              "  (conv1): Conv1d(10, 5, kernel_size=(5,), stride=(1,), padding=(2,))\n",
              "  (lstm): LSTM(15, 6, bidirectional=True)\n",
              "  (hidden2tag): Linear(in_features=12, out_features=47, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FeyETK3xofd"
      },
      "source": [
        "test_dataset = read_dataset(\"corpus.answer\", with_tags=True)"
      ],
      "id": "1FeyETK3xofd",
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRXXl1VCy97Y",
        "outputId": "244d920f-49a4-4d35-ed46-845ad71d9320"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "sentence = test_dataset[2]\n",
        "words, taggs = sentence\n",
        "# print(words)\n",
        "target = prepare_sequence(taggs, tag_to_idx, absent_key=ABSENT_WORD[0], random_key=None)\n",
        "codded_sentence = prepare_sequence(words, word_to_idx, absent_key=ABSENT_WORD[0], random_key=None)\n",
        "codded_words = []\n",
        "\n",
        "max_word_len = max(len(word) for word in words)\n",
        "for word in words:\n",
        "  paddedWord = padd_word(word, length = max_word_len, symbol=' ')\n",
        "  codded_word = prepare_sequence(paddedWord, char_to_idx, absent_key=PADDING_CHAR)\n",
        "  codded_word = torch.reshape(codded_word, (1,-1))\n",
        "  codded_words.append(codded_word)\n",
        "\n",
        "words_ = torch.cat(codded_words,dim=0)\n",
        "tag_scores = model(codded_sentence, words_)\n",
        "pred = torch.argmax(tag_scores, dim=1)\n",
        "print(accuracy_score(target, pred))"
      ],
      "id": "qRXXl1VCy97Y",
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8666666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0t0o-om5gMp",
        "outputId": "2015ba47-e0e3-44c6-8c17-ede04d5faafc"
      },
      "source": [
        "word_to_idx[ABSENT_WORD[0]]"
      ],
      "id": "z0t0o-om5gMp",
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38472"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1oMtgx3yDf0"
      },
      "source": [
        "y_true = []\n",
        "y_pred = []\n",
        "for (idx, sentence) in enumerate(test_dataset):\n",
        "  words, taggs = sentence\n",
        "\n",
        "  target = prepare_sequence(taggs, tag_to_idx, absent_key=ABSENT_WORD[0], random_key=None)\n",
        "  codded_sentence = prepare_sequence(words, word_to_idx, absent_key=ABSENT_WORD[0], random_key=None)\n",
        "  codded_words = []\n",
        "  max_word_len = max(len(word) for word in words)\n",
        "  for word in words:\n",
        "    paddedWord = padd_word(word, length = max_word_len, symbol=' ')\n",
        "    codded_word = prepare_sequence(paddedWord, char_to_idx, absent_key=PADDING_CHAR)\n",
        "    codded_word = torch.reshape(codded_word, (1,-1))\n",
        "    codded_words.append(codded_word)\n",
        "\n",
        "  words_ = torch.cat(codded_words,dim=0)\n",
        "  tag_scores = model(codded_sentence, words_)\n",
        "  pred = torch.argmax(tag_scores, dim=1)\n",
        "  y_true += target.tolist()\n",
        "  y_pred += pred.tolist()\n"
      ],
      "id": "b1oMtgx3yDf0",
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iY7fzte_yKGu",
        "outputId": "3e8ea018-6f87-4ad7-d389-dd682eecaf0b"
      },
      "source": [
        "print(accuracy_score(y_true, y_pred))"
      ],
      "id": "iY7fzte_yKGu",
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8718248626222056\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tCaqX5y4nw6"
      },
      "source": [
        ""
      ],
      "id": "2tCaqX5y4nw6",
      "execution_count": null,
      "outputs": []
    }
  ]
}